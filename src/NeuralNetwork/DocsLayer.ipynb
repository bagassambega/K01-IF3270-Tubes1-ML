{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Layer as ffp\n",
    "import Backpropagation as bp\n",
    "import Lossfunction as lf\n",
    "\n",
    "import importlib\n",
    "importlib.reload(ffp)\n",
    "importlib.reload(bp)\n",
    "importlib.reload(lf)\n",
    "\n",
    "from Layer import *\n",
    "from Backpropagation import *\n",
    "from Lossfunction import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contoh penggunaan kode\n",
    "pada kode di bawah ini adalah cara pemanggilan untuk Layer dan melakukan forward propagation\n",
    "\n",
    "inputs: np.array 2Dimensi \n",
    "\n",
    "untuk memanggil Layer baru kita bisa memanggil Dense \n",
    "Dense: \n",
    "    neurons: jumlah neuron yang ada pada layer tersebut\n",
    "    activation: activation function yang akan digunakan untuk output dari layer tersebut \n",
    "    inputShape: inputya dalam dimensi berapa dalam contoh dibawah digunakan (3,) yang berarti matriks 3 x 1 (row x col)\n",
    "\n",
    "Untuk melakukan feed forward propagation kita bisa memanggil fungsi method call\n",
    "```python\n",
    "outputs = dense_layer.call(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m]])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a Dense layer with 4 neurons, 'relu' activation, and input shape (3,)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m dense_layer \u001b[38;5;241m=\u001b[39m \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneurons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputShape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m dense_layer\u001b[38;5;241m.\u001b[39mcall(inputs)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer hidden 1 output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs)\n",
      "File \u001b[1;32md:\\Indra\\Programming\\College\\ML\\K01-IF3270-Tubes1-ML\\src\\NeuralNetwork\\Layer.py:107\u001b[0m, in \u001b[0;36mDense.__init__\u001b[1;34m(self, neurons, activation, inputShape, inputWeight, weightInit, weightInitParams)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweightInitParams \u001b[38;5;241m=\u001b[39m weightInitParams \u001b[38;5;28;01mif\u001b[39;00m weightInitParams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# print(weightInit)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# print(\"weight init: \", self.weightInitParams)\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Indra\\Programming\\College\\ML\\K01-IF3270-Tubes1-ML\\src\\NeuralNetwork\\Layer.py:130\u001b[0m, in \u001b[0;36mDense.create\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m     weightsArr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputWeight\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# The first row is the bias and the remaining rows are the weights.\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m \u001b[43mweightsArr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m weightsArr[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWEIGHT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Use a tuple (3,) for inputShape instead of (3)\n",
    "inputs = np.array([[0.5, 0.2, -0.1]])\n",
    "\n",
    "# Create a Dense layer with 4 neurons, 'relu' activation, and input shape (3,)\n",
    "dense_layer = Dense(neurons=4, activation='relu', inputShape=(3,))\n",
    "outputs = dense_layer.call(inputs)\n",
    "print(\"Layer hidden 1 output:\", outputs)\n",
    "\n",
    "# Create a Dense layer with 2 neurons, 'sigmoid' activation, and input shape (4,)\n",
    "dense_layer2 = Dense(neurons=2, activation='sigmoid', inputShape=(4,))\n",
    "outputs2 = dense_layer2.call(outputs)\n",
    "print(\"Layer output:\", outputs2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## contoh penggunaan untuk inisialisi bobot sesuai spesifikasi \n",
    "\n",
    "contoh 1: bobot default \n",
    "\n",
    "contoh 2: untuk pemanggilan menggunakan uniform method \n",
    "\n",
    "contoh 3: untuk pemanggilan menggunkan uniform normal \n",
    "\n",
    "contoh 4: untuk zero inisialisasi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default\n",
      "weight init:  {}\n",
      "LHO KOK KESINI\n",
      "Layer hidden 1 output (default/Xavier): [[0.         0.         0.81904577 0.3449505 ]]\n",
      "uniform\n",
      "weight init:  {'lower_bound': -0.1, 'upper_bound': 0.1, 'seed': 42}\n",
      "LHO KOK KESINI\n",
      "Layer hidden 1 output (uniform): [[0.08576399 0.         0.         0.        ]]\n",
      "normal\n",
      "weight init:  {'mean': 0, 'variance': 0.01, 'seed': 42}\n",
      "LHO KOK KESINI\n",
      "Layer hidden 1 output (normal): [[0.07582605 0.         0.21647917 0.33928599]]\n",
      "zero\n",
      "weight init:  {}\n",
      "LHO KOK KESINI\n",
      "Layer hidden 1 output (zero): [[0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Contoh Penggunaan ---\n",
    "\n",
    "# Contoh 1: Inisialisasi dengan metode default (misalnya Xavier uniform)\n",
    "inputs = np.array([[0.5, 0.2, -0.1]])\n",
    "dense_layer = Dense(neurons=4, activation='relu', inputShape=(3,))\n",
    "outputs = dense_layer.call(inputs)\n",
    "print(\"Layer hidden 1 output (default/Xavier):\", outputs)\n",
    "\n",
    "# Contoh 2: Inisialisasi dengan metode uniform dengan parameter khusus\n",
    "weightInitParams = {'lower_bound': -0.1, 'upper_bound': 0.1, 'seed': 42}\n",
    "dense_layer_uniform = Dense(neurons=4, activation='relu', inputShape=(3,),\n",
    "                            weightInit='uniform', weightInitParams=weightInitParams)\n",
    "outputs_uniform = dense_layer_uniform.call(inputs)\n",
    "print(\"Layer hidden 1 output (uniform):\", outputs_uniform)\n",
    "\n",
    "# Contoh 3: Inisialisasi dengan metode normal dengan parameter khusus\n",
    "weightInit_params_normal = {'mean': 0, 'variance': 0.01, 'seed': 42}\n",
    "dense_layer_normal = Dense(neurons=4, activation='relu', inputShape=(3,),\n",
    "                           weightInit='normal', weightInitParams=weightInit_params_normal)\n",
    "outputs_normal = dense_layer_normal.call(inputs)\n",
    "print(\"Layer hidden 1 output (normal):\", outputs_normal)\n",
    "\n",
    "# Contoh 4: Inisialisasi dengan zero initialization\n",
    "dense_layer_zero = Dense(neurons=4, activation='relu', inputShape=(3,),\n",
    "                         weightInit='zero')\n",
    "outputs_zero = dense_layer_zero.call(inputs)\n",
    "print(\"Layer hidden 1 output (zero):\", outputs_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contoh print untuk tiap layer\n",
    "\n",
    "Gunakan kode dibawah untuk merepresentasikan bias dan bobot pada layar\n",
    "```python\n",
    "print(dense_layer.__repr__())\n",
    "```\n",
    "\n",
    "\n",
    "atau untuk print satu-satu bisa digunakan untuk print per attributenya\n",
    "```python\n",
    "print(dense_layer.activation.__name__) # print dense_layer activation\n",
    "print(dense_layer.bias) # print bias\n",
    "print(dense_layer.weight) # print bobot\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default\n",
      "weight init:  {}\n",
      "LHO KOK KESINI\n",
      "Layer hidden 1: [[0.         0.43429734 0.79032172 0.        ]]\n",
      "Layer: \n",
      "activation = relu\n",
      "weights \n",
      " = [[ 0.56626722  0.36688287 -0.29579857 -0.63792801]\n",
      " [ 0.8465941  -0.30256811 -0.75408817 -0.74673617]\n",
      " [ 0.64343454  0.19206329  0.56869106  0.42538061]]\n",
      "bias = [-0.51764338  0.16528793  0.57295387 -0.91378674]\n",
      "\n",
      "default\n",
      "weight init:  {}\n",
      "LHO KOK KESINI\n",
      "Layer output: [[0.73160111 0.89260925]]\n",
      "Layer: \n",
      "activation = sigmoid\n",
      "weights \n",
      " = [[-0.24293125  0.10408126]\n",
      " [ 0.65880933  0.2370395 ]\n",
      " [ 0.7234138   0.15470429]\n",
      " [ 0.40914367 -0.90835123]]\n",
      "bias = [0.07245618 0.94623153]\n",
      "\n",
      "name: relu\n",
      "bias: [-0.51764338  0.16528793  0.57295387 -0.91378674]\n",
      "weight: [[ 0.56626722  0.36688287 -0.29579857 -0.63792801]\n",
      " [ 0.8465941  -0.30256811 -0.75408817 -0.74673617]\n",
      " [ 0.64343454  0.19206329  0.56869106  0.42538061]]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[0.5, 0.2, -0.1]])\n",
    "\n",
    "dense_layer = Dense(neurons=4, activation='relu', inputShape=(3,))\n",
    "outputs = dense_layer.call(inputs)\n",
    "print(\"Layer hidden 1:\", outputs)\n",
    "print(dense_layer.__repr__())\n",
    "\n",
    "dense_layer2 = Dense(neurons=2, activation='sigmoid', inputShape=(4,))\n",
    "outputs2 = dense_layer2.call(outputs)\n",
    "print(\"Layer output:\", outputs2)\n",
    "print(dense_layer2.__repr__())\n",
    "\n",
    "# --- Contoh Penggunaan per Attribute---\n",
    "\n",
    "print(f\"name: {dense_layer.activation.__name__}\") # Get activation\n",
    "print(f\"bias: {dense_layer.bias}\") # Get biases\n",
    "print(f\"weight: {dense_layer.weights}\") # Get weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer hidden 1 output:\n",
      " [[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [2. 1.]]\n",
      "Layer output:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#contoh forward yang ada di kelas\n",
    "\n",
    "inputs = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1], \n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "weight_input = np.array([\n",
    "    [0, -1],\n",
    "    [1, 1],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "\n",
    "hidden_layer = Dense(neurons=2, activation='relu', inputShape=(2,), inputWeight=weight_input)\n",
    "outputs = hidden_layer.call(inputs)\n",
    "\n",
    "print(\"Layer hidden 1 output:\\n\", outputs)\n",
    "\n",
    "weight_hidden = np.array([\n",
    "    [0],\n",
    "    [1], \n",
    "    [-2]\n",
    "])\n",
    "\n",
    "output_layer = Dense(neurons=1, activation='relu', inputShape=(2,), inputWeight=weight_hidden)\n",
    "outputs = output_layer.call(outputs)\n",
    "\n",
    "print(\"Layer output:\\n\", outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHT\n",
      "[[ 0.01558781  0.03802653  0.0106119  -0.02329119]\n",
      " [ 0.04632484  0.01528559  0.05624191  0.03231619]\n",
      " [ 0.0678596  -0.0121211  -0.06899012 -0.07003299]]\n",
      "WEIGHT\n",
      "[[ 0.08276669  0.01558781  0.03802653]\n",
      " [ 0.0106119  -0.02329119  0.04632484]\n",
      " [ 0.01528559  0.05624191  0.03231619]\n",
      " [ 0.0678596  -0.0121211  -0.06899012]]\n",
      "WEIGHT\n",
      "[[-0.06679266  0.08276669]\n",
      " [ 0.01558781  0.03802653]\n",
      " [ 0.0106119  -0.02329119]]\n",
      "\n",
      "Predictions before training:\n",
      "[[0.49405132 0.48653921]\n",
      " [0.49405132 0.48653921]\n",
      " [0.49405132 0.48653921]\n",
      " [0.49405132 0.48653921]\n",
      " [0.49405132 0.48653921]]\n",
      "\n",
      "Predictions on training data:\n",
      "[[0.49471147 0.48600067]\n",
      " [0.49471147 0.48600067]\n",
      " [0.49471147 0.48600067]\n",
      " [0.49471147 0.48600067]\n",
      " [0.49471147 0.48600067]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    weight_init_params = {'lower_bound': -0.1, 'upper_bound': 0.1, 'seed': 41} \n",
    "    hidden_layer = Dense(neurons=4, activation=\"relu\", inputShape=(3,), weightInit=\"uniform\", weightInitParams=weight_init_params)\n",
    "    hidden_layer2 = Dense(neurons=3, activation=\"relu\", inputShape=(4,), weightInit=\"uniform\", weightInitParams=weight_init_params)\n",
    "    output_layer = Dense(neurons=2, activation=\"sigmoid\", inputShape=(3,), weightInit=\"uniform\", weightInitParams=weight_init_params)\n",
    "\n",
    "    \n",
    "    # Create and compile the backpropagation optimizer\n",
    "    bp = Backpropagation(\n",
    "        learning_rate=0.01,\n",
    "        batch_size=1000, \n",
    "        epochs=10, \n",
    "        loss='mse',\n",
    "        verbose=False\n",
    "        )\n",
    "\n",
    "    bp.compile([hidden_layer, hidden_layer2 ,output_layer])\n",
    "    \n",
    "    # Create a batch of training samples\n",
    "    x_train = np.array([\n",
    "        [0.5, -0.2, 0.1],\n",
    "        [0.1, 0.4, -0.3],\n",
    "        [-0.3, 0.2, 0.6],\n",
    "        [0.7, -0.8, 0.9],\n",
    "        [0.2, 0.5, -0.4]\n",
    "    ])\n",
    "    y_train = np.array([\n",
    "        [1, 0],\n",
    "        [0, 1],\n",
    "        [1, 0],\n",
    "        [0, 1],\n",
    "        [1, 0]\n",
    "    ])\n",
    "\n",
    "    predictions = bp.predict(x_train)\n",
    "    print(\"\\nPredictions before training:\")\n",
    "    print(predictions) \n",
    "\n",
    "    history = bp.train(\n",
    "        x_train, \n",
    "        y_train, \n",
    "    )\n",
    "    \n",
    "    predictions = bp.predict(x_train)\n",
    "    print(\"\\nPredictions on training data:\")\n",
    "    print(predictions)\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
